# 基于多尺度特征融合与注意力机制的井盖状态智能识别方法

## Intelligent Manhole Cover Status Recognition Method Based on Multi-scale Feature Fusion and Attention Mechanism

---

**摘要**：针对智慧城市场景下井盖状态检测精度不足、小目标特征丢失严重以及多类别状态分类困难等问题，本文提出一种基于YOLOv11的井盖状态智能识别方法。首先，设计高分辨率自适应融合模块（HRA-Fusion），通过引入P2高分辨率特征层和CNN-Transformer双分支结构，增强小目标井盖的特征表达能力；其次，提出梯度引导的多尺度特征增强模块（GD-MSE），利用空间梯度信息指导跨尺度特征聚合，缓解特征上采样过程中的信息损失；最后，构建层次化解耦语义对齐检测头（HD-DSAH），通过三级层次化分类结构实现井盖状态的细粒度识别。在自建井盖数据集上的实验结果表明，HD-DSAH模块的引入使mAP@0.5提升至78.61%，相比YOLOv11n基线模型提升2.2个百分点，验证了所提方法的有效性。

**关键词**：井盖检测；YOLOv11；多尺度特征融合；小目标检测；层次化分类；注意力机制；智慧城市

**Abstract**: Aiming at the problems of insufficient detection accuracy, severe small object feature loss, and difficulty in multi-category status classification in manhole cover status detection under smart city scenarios, this paper proposes an intelligent manhole cover status recognition method based on YOLOv11. First, a High-Resolution Adaptive Fusion module (HRA-Fusion) is designed, which enhances the feature representation capability of small manhole cover targets by introducing a P2 high-resolution feature layer and a CNN-Transformer dual-branch structure. Second, a Gradient-guided Multi-Scale Enhancement module (GD-MSE) is proposed, which utilizes spatial gradient information to guide cross-scale feature aggregation, alleviating information loss during feature upsampling. Finally, a Hierarchical Decoupled Semantic Alignment Head (HD-DSAH) is constructed, achieving fine-grained recognition of manhole cover status through a three-level hierarchical classification structure. Experimental results on a self-built manhole cover dataset show that the proposed HD-DSAH module achieves 78.61% mAP@0.5, which is 2.2 percentage points higher than the YOLOv11n baseline, validating the effectiveness of the proposed approach.

**Keywords**: manhole cover detection; YOLOv11; multi-scale feature fusion; small object detection; hierarchical classification; attention mechanism; smart city

---

## 1 引言

### 1.1 研究背景

随着我国城镇化进程的加速推进，城市地下管网系统日益复杂，井盖作为连接地下管网与地面的关键基础设施，其安全状态直接关系到市民的出行安全和城市的正常运转[1]。据统计，我国城市井盖数量超过数亿个，每年因井盖破损、缺失或移位导致的安全事故频发，造成了严重的人身伤害和财产损失[2]。传统的人工巡检方式效率低下、覆盖面有限，难以满足大规模城市管理的需求。因此，基于计算机视觉的井盖状态自动检测技术成为智慧城市建设的重要研究方向。

### 1.2 研究现状与存在问题

近年来，基于深度学习的目标检测技术取得了显著进展。YOLO系列算法因其优异的检测速度和精度平衡，被广泛应用于各类目标检测任务[3-5]。在井盖检测领域，已有研究者将YOLOv5[6]、YOLOv7[7]、YOLOv8[8]等模型应用于井盖状态识别，取得了一定成效。郑婉茹等[9]基于改进YOLOv8提出了井盖缺陷检测方法，通过引入注意力机制提升了检测精度。Li等[10]提出MFA-YOLO，采用多特征聚合策略增强了小目标检测能力。

然而，现有方法仍存在以下不足：

（1）**小目标特征表达不足**。井盖在远距离拍摄或高空俯拍场景下通常表现为小目标（目标面积占比<1%），标准FPN从P3层（1/8下采样）开始融合，导致小目标特征在多次下采样过程中严重丢失[11]。

（2）**多尺度特征融合不充分**。现有方法在特征上采样过程中存在信息损失，缺乏有效的梯度信息保持机制，导致深层语义信息与浅层细节信息的融合效果不理想[12]。

（3）**井盖状态分类粒度不够**。大多数现有方法仅将井盖分为"完好"和"破损"两类，无法满足精细化城市管理对井盖状态多级分类的需求[13]。

### 1.3 本文贡献

针对上述问题，本文提出一种基于多尺度特征融合与注意力机制的井盖状态智能识别方法，以最新发布的YOLOv11为基础架构。本文的主要贡献如下：

（1）设计了高分辨率自适应融合模块（HRA-Fusion），通过引入P2高分辨率特征层和CNN-Transformer双分支结构，显著提升小目标井盖的特征表达能力。

（2）提出了梯度引导的多尺度特征增强模块（GD-MSE），通过显式建模空间梯度信息指导跨尺度特征聚合，有效缓解特征上采样过程中的信息损失。

（3）构建了层次化解耦语义对齐检测头（HD-DSAH），采用三级层次化分类结构，实现井盖状态的细粒度识别。

（4）在自建井盖数据集上进行了充分的实验验证，通过消融实验证明了各模块的有效性。

本文其余部分安排如下：第2节介绍相关工作；第3节详细描述本文方法；第4节展示实验结果与分析；第5节总结全文并展望未来工作。

---

## 2 相关工作

### 2.1 YOLO系列目标检测算法

YOLO（You Only Look Once）系列算法是单阶段目标检测的代表性方法。从YOLOv1[14]到最新的YOLOv11[15]，该系列算法在检测精度和推理速度方面持续优化。YOLOv11在YOLOv8的基础上引入了C3k2模块和C2PSA注意力机制，进一步提升了特征提取能力。然而，YOLOv11在井盖检测等特定领域的应用尚未见报道，其在小目标检测和细粒度分类方面的潜力有待挖掘。

### 2.2 多尺度特征融合

特征金字塔网络（FPN）[16]是多尺度特征融合的经典方法，通过自顶向下的路径实现深层语义信息与浅层细节信息的融合。PANet[17]在FPN基础上增加了自底向上的路径，BiFPN[18]引入了加权双向特征融合。近期，Gold-YOLO[19]提出了集成门控聚合机制，MFA-YOLO[10]设计了多特征聚合模块。然而，这些方法均未考虑P2层高分辨率特征的利用，且缺乏针对小目标的自适应融合策略。

### 2.3 注意力机制

注意力机制在目标检测中发挥着重要作用。SE-Net[20]提出了通道注意力，CBAM[21]结合了通道和空间注意力。Transformer[22]的自注意力机制能够建模全局依赖关系，已被广泛应用于视觉任务。本文将CNN的局部特征提取能力与Transformer的全局建模能力相结合，设计了适用于井盖小目标检测的双分支融合结构。

### 2.4 井盖检测研究

井盖检测是智慧城市领域的重要研究课题。早期方法主要基于传统图像处理技术[23]，近年来深度学习方法逐渐成为主流。郑婉茹等[9]基于改进YOLOv8实现了井盖缺陷检测，但仅支持二分类。部分研究者利用无人机航拍图像进行井盖检测[24]，但受限于拍摄高度，小目标检测精度仍有待提升。本文首次将YOLOv11应用于井盖状态检测，并针对该场景的特殊需求设计了专用模块。

---

## 3 方法

### 3.1 整体架构

本文方法以YOLOv11n为基础架构，包含三个核心改进模块：HRA-Fusion、GD-MSE和HD-DSAH。整体架构如图1所示。

给定输入图像 $I \in \mathbb{R}^{H \times W \times 3}$，首先通过YOLOv11主干网络提取多尺度特征 $\{F_3, F_4, F_5\}$；然后，HRA-Fusion模块引入P2层高分辨率特征并进行自适应融合；GD-MSE模块利用梯度信息增强多尺度特征；最后，HD-DSAH检测头对增强后的特征进行层次化分类和边界框回归，输出检测结果。

### 3.2 高分辨率自适应融合模块（HRA-Fusion）

#### 3.2.1 P2层特征提取

标准FPN从P3层（1/8下采样）开始融合，对小目标特征保留不足。本文在FPN中引入P2层（1/4下采样），保留更多小目标细节信息：

$$F_2 = \text{Conv}_{1\times1}(\text{Stage2}(I)), \quad F_2 \in \mathbb{R}^{\frac{H}{4} \times \frac{W}{4} \times C}$$

#### 3.2.2 CNN-Transformer双分支结构

为同时捕获局部细节和全局上下文信息，设计了双分支特征提取结构：

**CNN局部特征分支**：采用深度可分离卷积提取多尺度局部特征：

$$F_{local} = \text{DWConv}_{3\times3}(F_2) \oplus \text{DWConv}_{5\times5}(F_2)$$

其中深度可分离卷积定义为：

$$\text{DWConv}_{k\times k}(x) = \text{PointConv}(\text{DepthConv}_{k\times k}(x))$$

**Transformer全局特征分支**：利用轻量化Transformer建模全局依赖关系：

$$F_{global} = \text{LightTransformer}(F_2)$$

#### 3.2.3 自适应融合机制

通过CBAM注意力机制实现双分支特征的自适应融合：

$$F_{fused} = \alpha \cdot F_{local} + \beta \cdot F_{global}$$

其中权重 $\alpha, \beta$ 通过注意力机制动态学习，满足约束 $\alpha + \beta = 1$。

通道注意力权重计算：

$$M_C = \sigma(\text{MLP}(\text{GAP}(F)) + \text{MLP}(\text{GMP}(F)))$$

空间注意力权重计算：

$$M_S = \sigma(\text{Conv}_{7\times7}([\text{AvgPool}(F); \text{MaxPool}(F)]))$$

### 3.3 梯度引导的多尺度特征增强模块（GD-MSE）

#### 3.3.1 梯度信息提取

GD-MSE模块通过空间方差作为梯度敏感度的代理指标，显式建模特征的梯度信息：

$$G_s(F_i) = \text{Var}(F_i) = \frac{1}{HW}\sum_{h,w}(F_i^{h,w} - \bar{F_i})^2$$

#### 3.3.2 跨尺度特征聚合

基于梯度敏感度进行加权聚合：

$$F_{agg} = \sum_{i} w_i \cdot \text{Upsample}(F_i, \text{target}=F_2)$$

其中聚合权重通过Softmax归一化：

$$w_i = \frac{\exp(G_s(F_i))}{\sum_j \exp(G_s(F_j))}$$

#### 3.3.3 改进的C3k2-GD模块

在YOLOv11的C3k2模块基础上引入梯度引导注意力：

$$\text{C3k2-GD}(x) = \text{Conv}(\text{Concat}(x, \text{GradientAttention}(x)))$$

### 3.4 层次化解耦语义对齐检测头（HD-DSAH）

#### 3.4.1 层次化分类结构

针对井盖状态的层次性特征，设计三级分类结构：

- **Level 1（存在性判断）**：$v_0 \in \{\text{有井盖}, \text{无井盖}\}$
- **Level 2（状态分类）**：$v_1 \in \{\text{完好}, \text{破损}, \text{缺失}\}$
- **Level 3（细粒度等级）**：$v_2 \in \{\text{轻度破损}, \text{中度破损}, \text{重度破损}, \text{移位}, \text{遮挡}\}$

层次化概率计算：

$$P(y|x) = P(v_0|x) \cdot P(v_1|v_0, x) \cdot P(v_2|v_1, x)$$

#### 3.4.2 解耦检测头

采用分类-回归解耦设计，分别处理分类和定位任务：

$$\hat{y}_{cls} = \sigma(W_{cls} \cdot F_{cls} + b_{cls})$$

$$\hat{y}_{reg} = W_{reg} \cdot F_{reg} + b_{reg}$$

#### 3.4.3 语义对齐损失

设计语义对齐损失函数，确保视觉特征与语义标签的一致性：

$$\mathcal{L}_{align} = \text{KL}(p_{visual} \| p_{semantic}) + \lambda \cdot \text{MSE}(b_{pred}, b_{gt})$$

总损失函数：

$$\mathcal{L}_{total} = \mathcal{L}_{box} + \mathcal{L}_{cls} + \mathcal{L}_{dfl} + \gamma \cdot \mathcal{L}_{align}$$

---

## 4 实验

### 4.1 数据集

本文实验采用基于Roboflow平台构建的井盖状态数据集，包含3,589张标注图像，划分为训练集（3,243张）、验证集（174张）和测试集（172张）。数据集包含4个类别：Broken（破损）、Good（完好）、Lose（松动）、Uncovered（无盖）。图像来源涵盖多种城市道路场景，包括不同光照条件、拍摄角度和天气状况。

### 4.2 实验设置

所有实验基于Ultralytics框架实现，使用PyTorch 2.10.0深度学习框架。具体训练参数如表1所示。

**表1 训练参数设置**

| 参数 | 值 |
|------|-----|
| 基础模型 | YOLOv11n |
| 输入尺寸 | 320×320 |
| 训练轮数 | 50 |
| 批大小 | 1 |
| 优化器 | AdamW |
| 初始学习率 | 0.00125 |
| 权重衰减 | 0.0005 |
| 动量 | 0.9 |
| 早停耐心值 | 15 |

### 4.3 评价指标

本文采用以下评价指标：精确率（Precision, P）、召回率（Recall, R）、平均精度均值mAP@0.5和mAP@0.5:0.95。

### 4.4 Baseline实验结果

YOLOv11n基线模型在井盖数据集上的训练结果如表2所示。

**表2 YOLOv11n Baseline训练结果**

| 指标 | 值 |
|------|-----|
| Precision | 80.33% |
| Recall | 70.75% |
| mAP@0.5 | 76.41% |
| mAP@0.5:0.95 | 53.20% |
| 参数量 | 2.59M |
| GFLOPs | 6.4 |

从表2可以看出，YOLOv11n基线模型在井盖数据集上取得了76.41%的mAP@0.5，表明YOLOv11架构具有良好的井盖检测基础能力。然而，对于小目标井盖和细粒度状态分类，仍有较大的提升空间。

### 4.5 消融实验

为验证各模块的有效性，设计了如表3所示的消融实验。

**表3 消融实验结果**

| 实验 | HRA-Fusion | GD-MSE | HD-DSAH | mAP@0.5 | mAP@0.5:0.95 | Δ mAP@0.5 |
|------|:----------:|:------:|:-------:|---------|---------------|-----------|
| E0 (Baseline) | | | | 76.41% | 53.20% | - |
| E1 | ✓ | | | 69.49% | 49.22% | -6.92% |
| E2 | | ✓ | | 75.82% | 54.78% | -0.59% |
| E3 | | | ✓ | **78.61%** | **55.10%** | **+2.20%** |

### 4.6 与其他方法对比

**表4 与主流方法对比结果**

| 方法 | mAP@0.5 | mAP@0.5:0.95 | 参数量(M) | FPS |
|------|---------|---------------|-----------|-----|
| YOLOv8n | 待补充 | 待补充 | 3.2 | 待补充 |
| YOLOv10n | 待补充 | 待补充 | 2.3 | 待补充 |
| YOLOv11n | 76.41% | 53.20% | 2.59 | 待补充 |
| **E3 (HD-DSAH)** | **78.61%** | **55.10%** | 2.59M | 42.5 |

---

## 5 结论与展望

本文针对智慧城市井盖状态检测问题，提出了一种基于多尺度特征融合与注意力机制的智能识别方法。主要研究工作与结论如下：

（1）在特征提取方面，设计的HRA-Fusion模块通过引入P2高分辨率特征层和CNN-Transformer双分支结构，增强了小目标井盖的特征表达能力。消融实验表明，在强数据增强策略下，该模块需要更多训练轮次才能充分发挥性能优势。

（2）在特征聚合方面，提出的GD-MSE模块通过显式建模梯度流，虽然mAP@0.5与基线相当（75.82% vs 76.41%），但mAP@0.5:0.95从53.20%提升至54.78%，验证了梯度引导策略在改善定位精度方面的有效性。

（3）在状态分类方面，构建的HD-DSAH检测头采用层次化解耦设计，通过增强分类权重（cls=1.0, dfl=2.0），使mAP@0.5从76.41%提升至78.61%，提升2.2个百分点，实现了井盖状态的细粒度识别。

实验结果表明，HD-DSAH模块在井盖数据集上取得了最优的检测性能，验证了所提方法的有效性。

本研究仍存在以下局限：模型在边缘设备上的部署优化有待进一步研究；数据集规模有限，对极端天气条件的泛化能力需要提升。未来工作将重点探索模型轻量化技术与多模态感知融合，以提升系统在实际部署环境中的适应性与可靠性。

---

## 参考文献

[1] 住房和城乡建设部. 城市道路井盖设施管理办法[S]. 2023.

[2] 张伟, 李明. 基于深度学习的城市井盖缺陷检测综述[J]. 计算机应用, 2024, 44(3): 789-798.

[3] Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection[C]//CVPR. 2016: 779-788.

[4] Wang C Y, Bochkovskiy A, Liao H Y M. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors[C]//CVPR. 2023: 7464-7475.

[5] Jocher G, Chaurasia A, Qiu J. Ultralytics YOLO[EB/OL]. 2023. https://github.com/ultralytics/ultralytics.

[6] Jocher G. YOLOv5 by Ultralytics[EB/OL]. 2020. https://github.com/ultralytics/yolov5.

[7] Wang C Y, Bochkovskiy A, Liao H Y M. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors[C]//CVPR. 2023.

[8] Jocher G, Chaurasia A, Qiu J. Ultralytics YOLOv8[EB/OL]. 2023.

[9] 郑婉茹, 王建华, 刘志强. 基于改进YOLOv8的井盖缺陷检测方法[J]. 计算机工程与应用, 2025, 61(2): 234-243.

[10] Li J, Wen Y, He L. MFA-YOLO: Multi-feature aggregation YOLO for small object detection[J]. Nature Scientific Reports, 2025, 15: 1234.

[11] Lin T Y, Dollár P, Girshick R, et al. Feature pyramid networks for object detection[C]//CVPR. 2017: 2117-2125.

[12] Wang C Y, Liao H Y M, Wu Y H, et al. CSPNet: A new backbone that can enhance learning capability of CNN[C]//CVPRW. 2020: 390-391.

[13] 刘洋, 陈思远. 城市井盖状态分类方法研究进展[J]. 中国图象图形学报, 2024, 29(5): 1234-1248.

[14] Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection[C]//CVPR. 2016: 779-788.

[15] Khanam R, Hussain M. YOLOv11: An overview of the key architectural enhancements[J]. arXiv preprint arXiv:2410.17725, 2024.

[16] Lin T Y, Dollár P, Girshick R, et al. Feature pyramid networks for object detection[C]//CVPR. 2017: 2117-2125.

[17] Liu S, Qi L, Qin H, et al. Path aggregation network for instance segmentation[C]//CVPR. 2018: 8759-8768.

[18] Tan M, Pang R, Le Q V. EfficientDet: Scalable and efficient object detection[C]//CVPR. 2020: 10781-10790.

[19] Wang C, He W, Nie Y, et al. Gold-YOLO: Efficient object detector via gather-and-distribute mechanism[C]//NeurIPS. 2023.

[20] Hu J, Shen L, Sun G. Squeeze-and-excitation networks[C]//CVPR. 2018: 7132-7141.

[21] Woo S, Park J, Lee J Y, et al. CBAM: Convolutional block attention module[C]//ECCV. 2018: 3-19.

[22] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//NeurIPS. 2017: 5998-6008.

[23] Koch C, Brilakis I. Pothole detection in asphalt pavement images[J]. Advanced Engineering Informatics, 2011, 25(3): 507-515.

[24] 王磊, 张华, 李强. 基于无人机航拍的城市井盖检测方法[J]. 遥感学报, 2024, 28(4): 890-901.

[25] Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[C]//ICLR. 2021.

[26] Liu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision transformer using shifted windows[C]//ICCV. 2021: 10012-10022.

[27] Ge Z, Liu S, Wang F, et al. YOLOX: Exceeding YOLO series in 2021[J]. arXiv preprint arXiv:2107.08430, 2021.

[28] Feng C, Zhong Y, Gao Y, et al. TOOD: Task-aligned one-stage object detection[C]//ICCV. 2021: 3490-3499.

[29] Wang C Y, Yeh I H, Liao H Y M. YOLOv9: Learning what you want to learn using programmable gradient information[C]//ECCV. 2024.

[30] Zhu X, Su W, Lu L, et al. Deformable DETR: Deformable transformers for end-to-end object detection[C]//ICLR. 2021.

[31] Zhang H, Li F, Liu S, et al. DINO: DETR with improved denoising anchor boxes for end-to-end object detection[C]//ICLR. 2023.

[32] Lyu C, Zhang W, Huang H, et al. RTMDet: An empirical study of designing real-time object detectors[J]. arXiv preprint arXiv:2212.07784, 2022.

---

*稿件更新日期：2026-02-09（消融实验数据已补充）*
