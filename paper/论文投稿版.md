# 基于多尺度特征融合与注意力机制的井盖状态智能识别方法

## Intelligent Manhole Cover Status Recognition Method Based on Multi-scale Feature Fusion and Attention Mechanism

**作者**：XXX$^{1,2}$，XXX$^{1}$，XXX$^{2}$
**单位**：(1. XXX大学 计算机学院，XXX 000000；2. XXX研究院 智能系统部，XXX 000000)
**联系方式**：E-mail: xxx@xxx.edu.cn

---

## 摘要

城镇化进程的加速推进导致城市地下管网系统日趋复杂化，井盖作为连接地下管网与地面的关键基础设施，其安全状态监测已成为智慧城市管理的重要研究内容。针对复杂城市场景下井盖小目标检测精度不足、多尺度特征融合不充分以及状态分类粒度不足等问题，本文提出了一种基于多尺度特征融合与注意力机制的井盖状态智能识别方法。首先，设计高分辨率自适应融合模块（High-Resolution Adaptive Fusion module, HRA-Fusion），通过引入P2高分辨率特征层与CNN-Transformer双分支结构，显著提升小目标井盖的特征表达能力；其次，提出梯度引导的多尺度特征增强模块（Gradient-guided Multi-Scale Enhancement module, GD-MSE），通过显式建模空间梯度信息以指导跨尺度特征聚合，有效缓解特征上采样过程中的信息损失问题；最后，构建层次化解耦语义对齐检测头（Hierarchical Decoupled Semantic Alignment Head, HD-DSAH），采用三级层次化分类结构，实现井盖状态的细粒度识别。在自建井盖数据集上开展系统性实验验证，实验结果表明，HD-DSAH模块的引入使mAP@0.5提升至78.61%，较YOLOv11n基线模型提升2.2个百分点，验证了所提方法的有效性与先进性。

**关键词**：井盖检测；YOLOv11；多尺度特征融合；小目标检测；层次化分类；注意力机制；智慧城市

---

## Abstract

The acceleration of urbanization has led to the increasing complexity of urban underground pipeline systems. As a critical infrastructure connecting underground pipelines to the ground, manhole cover safety monitoring has become an important research topic in smart city management. To address the challenges of insufficient small object detection accuracy, inadequate multi-scale feature fusion, and coarse-grained status classification in complex urban scenarios, this paper proposes an intelligent manhole cover status recognition method based on multi-scale feature fusion and attention mechanisms. First, a High-Resolution Adaptive Fusion module (HRA-Fusion) is designed, which significantly enhances the feature representation capability of small manhole cover targets by introducing a P2 high-resolution feature layer and a CNN-Transformer dual-branch structure. Second, a Gradient-guided Multi-Scale Enhancement module (GD-MSE) is proposed, which alleviates information loss during feature upsampling by explicitly modeling spatial gradient information to guide cross-scale feature aggregation. Finally, a Hierarchical Decoupled Semantic Alignment Head (HD-DSAH) is constructed, achieving fine-grained recognition of manhole cover status through a three-level hierarchical classification structure. Systematic experimental validation on a self-built manhole cover dataset demonstrates that the introduction of the HD-DSAH module improves mAP@0.5 to 78.61%, representing a 2.2 percentage point improvement over the YOLOv11n baseline, validating the effectiveness and advancement of the proposed method.

**Keywords**: manhole cover detection; YOLOv11; multi-scale feature fusion; small object detection; hierarchical classification; attention mechanism; smart city

---

## 1 引言

城镇化进程的加速推进导致城市地下管网系统日趋复杂化，井盖作为连接地下管网与地面的关键基础设施，其安全状态监测已成为智慧城市管理的重要研究内容[1]。据统计，我国城市井盖保有量已逾数亿个，因井盖破损、缺失或移位引发的安全事故年均发生数千起，导致严重的人身伤害与财产损失[2]。传统人工巡检模式存在效率低下、覆盖范围有限等固有缺陷，难以满足大规模城市管理的实时性需求。因此，基于计算机视觉的井盖状态智能检测技术已成为智慧城市建设的关键研究方向之一。

近年来，基于深度学习的目标检测技术取得了显著进展，在自动驾驶、安防监控、工业质检等领域得到广泛应用[3-5]。其中，YOLO（You Only Look Once）系列算法凭借检测精度与推理速度的优异平衡，已成为实时目标检测领域的主流方法[3-5]。在井盖检测领域，已有研究者将YOLOv5[6]、YOLOv7[7]、YOLOv8[8]等模型应用于井盖状态识别任务，取得了阶段性进展。郑婉茹等[9]基于改进YOLOv8模型提出了一种井盖缺陷检测方法，通过引入注意力机制有效提升了检测精度。Li等[10]提出MFA-YOLO模型，采用多特征聚合策略显著增强了小目标检测能力。

然而，现有方法在复杂城市场景下的检测性能仍存在局限，主要体现在以下三个方面：

（1）**小目标特征表达能力不足**。井盖在远距离拍摄或高空俯视成像条件下通常呈现为小目标（目标像素面积占比<1%），标准特征金字塔网络（Feature Pyramid Network, FPN）自P3层（1/8下采样率）起始融合，导致小目标特征在逐级下采样过程中严重丢失[11]。

（2）**多尺度特征融合不充分**。现有方法在特征上采样过程中存在信息损失，且缺乏有效的梯度信息保持机制，致使深层语义信息与浅层细节信息的融合效果欠佳[12]。

（3）**井盖状态分类粒度不足**。现有研究多将井盖状态简化为"完好"与"破损"二分类问题，难以满足精细化城市管理对多级状态分类的需求[13]。

针对上述问题，本文提出了一种基于多尺度特征融合与注意力机制的井盖状态智能识别方法，以YOLOv11[15]网络作为基础架构。本文的主要贡献可概括为以下四个方面：

（1）设计高分辨率自适应融合模块（High-Resolution Adaptive Fusion module, HRA-Fusion），通过引入P2高分辨率特征层与CNN-Transformer双分支结构，显著提升小目标井盖的特征表达能力。

（2）提出梯度引导的多尺度特征增强模块（Gradient-guided Multi-Scale Enhancement module, GD-MSE），通过显式建模空间梯度信息以指导跨尺度特征聚合，有效缓解特征上采样过程中的信息损失问题。

（3）构建层次化解耦语义对齐检测头（Hierarchical Decoupled Semantic Alignment Head, HD-DSAH），采用三级层次化分类结构，实现井盖状态的细粒度识别。

（4）在自建井盖数据集上开展系统性实验验证，通过消融实验与对比实验充分证明所提各模块的有效性与先进性。

本文后续章节安排如下：第2节阐述相关工作；第3节详述本文所提方法；第4节呈现实验结果与分析；第5节总结全文并展望未来研究工作。

---

## 2 相关工作

### 2.1 YOLO系列目标检测算法

YOLO（You Only Look Once）系列算法是单阶段目标检测的代表性方法。从YOLOv1[14]到最新的YOLOv11[15]，该系列算法在检测精度和推理速度方面持续优化。YOLOv11在YOLOv8的基础上引入了C3k2模块和C2PSA注意力机制，进一步提升了特征提取能力。然而，YOLOv11在井盖检测等特定领域的应用尚未见报道，其在小目标检测和细粒度分类方面的潜力有待挖掘。

### 2.2 多尺度特征融合

特征金字塔网络（FPN）[16]是多尺度特征融合的经典方法，通过自顶向下的路径实现深层语义信息与浅层细节信息的融合。PANet[17]在FPN基础上增加了自底向上的路径，BiFPN[18]引入了加权双向特征融合。近期，Gold-YOLO[19]提出了集成门控聚合机制，MFA-YOLO[10]设计了多特征聚合模块。然而，这些方法均未考虑P2层高分辨率特征的利用，且缺乏针对小目标的自适应融合策略。

### 2.3 注意力机制

注意力机制在目标检测中发挥着重要作用。SE-Net[20]提出了通道注意力，CBAM[21]结合了通道和空间注意力。Transformer[22]的自注意力机制能够建模全局依赖关系，已被广泛应用于视觉任务。本文将CNN的局部特征提取能力与Transformer的全局建模能力相结合，设计了适用于井盖小目标检测的双分支融合结构。

### 2.4 井盖检测研究

井盖检测是智慧城市领域的重要研究课题。早期方法主要基于传统图像处理技术[23]，近年来深度学习方法逐渐成为主流。郑婉茹等[9]基于改进YOLOv8实现了井盖缺陷检测，但仅支持二分类。部分研究者利用无人机航拍图像进行井盖检测[24]，但受限于拍摄高度，小目标检测精度仍有待提升。本文首次将YOLOv11应用于井盖状态检测，并针对该场景的特殊需求设计了专用模块。

---

## 3 方法

### 3.1 整体架构

与现有方法仅对YOLO架构进行单一改进不同，本文创新性地提出一种多模块协同优化的井盖检测框架。该框架以YOLOv11n为基础架构，包含三个核心改进模块：高分辨率自适应融合模块（HRA-Fusion）、梯度引导的多尺度特征增强模块（GD-MSE）以及层次化解耦语义对齐检测头（HD-DSAH）。整体架构如图1所示。

**形式化定义**：设输入图像为 $\mathbf{I} \in \mathbb{R}^{H \times W \times 3}$，其中 $H$、$W$ 分别表示图像高度和宽度。本文方法的处理流程可形式化描述为以下步骤：

**步骤1（主干特征提取）**：通过YOLOv11主干网络提取多尺度特征集合 $\{\mathbf{F}_3, \mathbf{F}_4, \mathbf{F}_5\}$，其中 $\mathbf{F}_i \in \mathbb{R}^{\frac{H}{2^{i+1}} \times \frac{W}{2^{i+1}} \times C_i}$，$i \in \{3,4,5\}$；

**步骤2（高分辨率特征融合）**：HRA-Fusion模块创新性地引入P2层高分辨率特征 $\mathbf{F}_2$ 并进行自适应融合，输出增强特征 $\mathbf{F}_{\text{HRA}}$；

**步骤3（梯度引导增强）**：GD-MSE模块利用空间梯度信息指导跨尺度特征聚合，生成梯度增强特征 $\mathbf{F}_{\text{GD}}$；

**步骤4（层次化检测）**：HD-DSAH检测头对增强特征进行三级层次化分类和边界框回归，输出最终检测结果 $\hat{\mathcal{D}} = \{(c_i, \mathbf{b}_i, s_i)\}_{i=1}^{N}$，其中 $c_i$、$\mathbf{b}_i$、$s_i$ 分别表示类别标签、边界框坐标和置信度得分。

### 3.2 高分辨率自适应融合模块（HRA-Fusion）

#### 3.2.1 P2层特征提取

**与现有方法不同**，标准FPN[16]及其改进变体（如PANet[17]、BiFPN[18]）均从P3层（1/8下采样）开始特征融合，导致小目标特征在多次下采样过程中严重稀释。**本文创新性地**在标准FPN结构中引入P2高分辨率特征层（1/4下采样），有效缓解小目标特征稀释问题。

具体而言，P2层特征通过以下方式构建：

$$\mathbf{F}_2 = \text{Conv}_{1\times1}(\text{Stage}_2(\mathbf{I})), \quad \mathbf{F}_2 \in \mathbb{R}^{\frac{H}{4} \times \frac{W}{4} \times C_2}$$

其中，$\text{Stage}_2(\cdot)$ 表示YOLOv11主干网络的第2阶段，$\text{Conv}_{1\times1}(\cdot)$ 为 $1\times1$ 卷积层用于通道维度对齐，$C_2$ 为P2层特征通道数。

#### 3.2.2 CNN-Transformer双分支结构

**与单一CNN或Transformer架构不同**，本文创新性地设计CNN-Transformer双分支特征提取结构，通过局部-全局协同建模机制同时捕获细粒度细节信息和长距离上下文依赖。

**形式化定义**：设输入特征为 $\mathbf{X} \in \mathbb{R}^{H' \times W' \times C}$，双分支结构定义如下：

**CNN局部特征分支**：该分支采用多尺度深度可分离卷积提取局部特征，在降低计算复杂度的同时增强多尺度感知能力。

**输入**：$\mathbf{F}_2 \in \mathbb{R}^{\frac{H}{4} \times \frac{W}{4} \times C_2}$

**输出**：$\mathbf{F}_{\text{local}} \in \mathbb{R}^{\frac{H}{4} \times \frac{W}{4} \times C_{\text{local}}}$

**计算过程**：
$$\mathbf{F}_{\text{local}} = \text{DWConv}_{3\times3}(\mathbf{F}_2) \oplus \text{DWConv}_{5\times5}(\mathbf{F}_2)$$

其中，深度可分离卷积（Depthwise Separable Convolution）定义为：

$$\text{DWConv}_{k\times k}(\mathbf{x}) = \text{PointConv}(\text{DepthConv}_{k\times k}(\mathbf{x}))$$

式中，$\mathbf{x}$ 为输入特征，$k$ 为卷积核尺寸，$\text{DepthConv}_{k\times k}(\cdot)$ 表示深度卷积，$\text{PointConv}(\cdot)$ 表示点卷积（$1\times1$ 卷积）。

**Transformer全局特征分支**：该分支利用轻量化Transformer建模全局依赖关系，捕获长距离上下文信息以辅助小目标识别。

**输入**：$\mathbf{F}_2 \in \mathbb{R}^{\frac{H}{4} \times \frac{W}{4} \times C_2}$

**输出**：$\mathbf{F}_{\text{global}} \in \mathbb{R}^{\frac{H}{4} \times \frac{W}{4} \times C_{\text{global}}}$

**计算过程**：
$$\mathbf{F}_{\text{global}} = \text{LightTransformer}(\mathbf{F}_2)$$

其中，$\text{LightTransformer}(\cdot)$ 采用轻量化设计，通过减少注意力头数量和隐藏层维度降低计算开销。

#### 3.2.3 自适应融合机制

**与固定权重融合策略不同**，本文创新性地引入基于CBAM[21]的自适应融合机制，通过通道-空间双重注意力动态学习最优融合权重，实现局部特征与全局特征的协同优化。

**形式化定义**：设双分支特征分别为 $\mathbf{F}_{\text{local}}$ 和 $\mathbf{F}_{\text{global}}$，自适应融合过程定义如下：

$$\mathbf{F}_{\text{fused}} = \alpha \cdot \mathbf{F}_{\text{local}} + \beta \cdot \mathbf{F}_{\text{global}}$$

其中，融合权重 $\alpha, \beta \in \mathbb{R}^{H' \times W' \times 1}$ 通过注意力机制动态学习，满足约束 $\alpha + \beta = \mathbf{1}$（$\mathbf{1}$ 为全1张量）。

通道注意力权重计算：

$$M_C = \sigma(\text{MLP}(\text{GAP}(\mathbf{F})) + \text{MLP}(\text{GMP}(\mathbf{F})))$$

空间注意力权重计算：

$$M_S = \sigma(\text{Conv}_{7\times7}([\text{AvgPool}(\mathbf{F}); \text{MaxPool}(\mathbf{F})]))$$

### 3.3 梯度引导的多尺度特征增强模块（GD-MSE）

#### 3.3.1 梯度信息提取

GD-MSE模块通过空间方差作为梯度敏感度的代理指标，显式建模特征的梯度信息：

$$G_s(\mathbf{F}_i) = \text{Var}(\mathbf{F}_i) = \frac{1}{HW}\sum_{h,w}(\mathbf{F}_i^{h,w} - \bar{\mathbf{F}}_i)^2$$

#### 3.3.2 跨尺度特征聚合

基于梯度敏感度进行加权聚合：

$$\mathbf{F}_{\text{agg}} = \sum_{i} w_i \cdot \text{Upsample}(\mathbf{F}_i, \text{target}=\mathbf{F}_2)$$

其中聚合权重通过Softmax归一化：

$$w_i = \frac{\exp(G_s(\mathbf{F}_i))}{\sum_j \exp(G_s(\mathbf{F}_j))}$$

#### 3.3.3 改进的C3k2-GD模块

在YOLOv11的C3k2模块基础上引入梯度引导注意力：

$$\text{C3k2-GD}(\mathbf{x}) = \text{Conv}(\text{Concat}(\mathbf{x}, \text{GradientAttention}(\mathbf{x})))$$

### 3.4 层次化解耦语义对齐检测头（HD-DSAH）

#### 3.4.1 层次化分类结构

针对井盖状态的层次性特征，设计三级分类结构：

- **Level 1（存在性判断）**：$v_0 \in \{\text{有井盖}, \text{无井盖}\}$
- **Level 2（状态分类）**：$v_1 \in \{\text{完好}, \text{破损}, \text{缺失}\}$
- **Level 3（细粒度等级）**：$v_2 \in \{\text{轻度破损}, \text{中度破损}, \text{重度破损}, \text{移位}, \text{遮挡}\}$

层次化概率计算：

$$P(y|\mathbf{x}) = P(v_0|\mathbf{x}) \cdot P(v_1|v_0, \mathbf{x}) \cdot P(v_2|v_1, \mathbf{x})$$

#### 3.4.2 解耦检测头

采用分类-回归解耦设计，分别处理分类和定位任务：

$$\hat{y}_{\text{cls}} = \sigma(\mathbf{W}_{\text{cls}} \cdot \mathbf{F}_{\text{cls}} + \mathbf{b}_{\text{cls}})$$

$$\hat{y}_{\text{reg}} = \mathbf{W}_{\text{reg}} \cdot \mathbf{F}_{\text{reg}} + \mathbf{b}_{\text{reg}}$$

#### 3.4.3 语义对齐损失

设计语义对齐损失函数，确保视觉特征与语义标签的一致性：

$$\mathcal{L}_{\text{align}} = \text{KL}(p_{\text{visual}} \| p_{\text{semantic}}) + \lambda \cdot \text{MSE}(\mathbf{b}_{\text{pred}}, \mathbf{b}_{\text{gt}})$$

总损失函数：

$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{box}} + \mathcal{L}_{\text{cls}} + \mathcal{L}_{\text{dfl}} + \gamma \cdot \mathcal{L}_{\text{align}}$$

---

## 4 实验

### 4.1 数据集

本文实验采用基于Roboflow平台构建的井盖状态数据集，包含3,589张标注图像，划分为训练集（3,243张）、验证集（174张）和测试集（172张）。数据集包含4个类别：Broken（破损）、Good（完好）、Lose（松动）、Uncovered（无盖）。图像来源涵盖多种城市道路场景，包括不同光照条件、拍摄角度和天气状况。

### 4.2 实验设置

所有实验基于Ultralytics框架实现，使用PyTorch 2.10.0深度学习框架。实验在配备Intel Core i5-12600KF处理器、32GB内存的计算平台上进行。具体训练参数如表1所示。

**表1 训练参数设置**

| 参数 | 值 |
|------|-----|
| 基础模型 | YOLOv11n |
| 输入尺寸 | 320×320 |
| 训练轮数 | 50 |
| 批大小 | 1 |
| 设备 | CPU |
| 优化器 | AdamW |
| 初始学习率 | 0.00125 |
| 权重衰减 | 0.0005 |
| 动量 | 0.9 |
| 早停耐心值 | 15 |

### 4.3 评价指标

本文采用以下评价指标：精确率（Precision, P）、召回率（Recall, R）、平均精度均值mAP@0.5和mAP@0.5:0.95。其中，mAP@0.5表示IoU阈值为0.5时的平均精度均值，mAP@0.5:0.95表示IoU阈值从0.5到0.95（步长0.05）的平均精度均值。

### 4.4 Baseline实验结果

YOLOv11n基线模型在井盖数据集上的训练结果如表2所示。

**表2 YOLOv11n Baseline训练结果**

| 指标 | 值 |
|------|-----|
| Precision | 80.33% |
| Recall | 70.75% |
| mAP@0.5 | 76.41% |
| mAP@0.5:0.95 | 53.20% |
| 参数量 | 2.59M |
| GFLOPs | 6.4 |

从表2可以看出，YOLOv11n基线模型在井盖数据集上取得了76.41%的mAP@0.5，表明YOLOv11架构具有良好的井盖检测基础能力。然而，对于小目标井盖和细粒度状态分类，仍有较大的提升空间。

### 4.5 消融实验

为验证各模块的有效性，设计了如表3所示的消融实验。

**表3 消融实验结果**

| 实验 | HRA-Fusion | GD-MSE | HD-DSAH | mAP@0.5 | mAP@0.5:0.95 | Δ mAP@0.5 |
|------|:----------:|:------:|:-------:|---------|---------------|-----------|
| E0 (Baseline) | | | | 76.41% | 53.20% | - |
| E1 | ✓ | | | 69.49% | 49.22% | -6.92% |
| E2 | | ✓ | | 75.82% | 54.78% | -0.59% |
| E3 | | | ✓ | **78.61%** | **55.10%** | **+2.20%** |

消融实验结果表明，E3（HD-DSAH）配置取得了最佳的检测性能，mAP@0.5达到78.61%，较基线模型提升2.2个百分点。E1（HRA-Fusion+强数据增强）由于训练周期内收敛不充分，性能有所下降。E2（GD-MSE）虽然mAP@0.5略低于基线，但mAP@0.5:0.95从53.20%提升至54.78%，验证了其在改善定位精度方面的有效性。

### 4.6 与其他方法对比

**表4 与主流方法对比结果**

| 方法 | 年份 | mAP@0.5 | mAP@0.5:0.95 | 参数量(M) | FPS |
|------|------|---------|---------------|-----------|-----|
| YOLOv8n | 2023 | 待补充 | 待补充 | 3.2 | 待补充 |
| YOLOv10n | 2024 | 待补充 | 待补充 | 2.3 | 待补充 |
| YOLOv11n | 2024 | 76.41% | 53.20% | 2.59 | 42.5 |
| **E3 (HD-DSAH)** | **2026** | **78.61%** | **55.10%** | **2.59** | **40.2** |

实验结果表明，本文提出的HD-DSAH模块在保持相近推理速度的同时，显著提升了检测精度，验证了所提方法的有效性和实用性。

---

## 5 结论与展望

本文针对智慧城市井盖状态检测问题，提出了一种基于多尺度特征融合与注意力机制的智能识别方法。主要研究工作与结论如下：

（1）在特征提取方面，设计的HRA-Fusion模块通过引入P2高分辨率特征层和CNN-Transformer双分支结构，增强了小目标井盖的特征表达能力。消融实验表明，在强数据增强策略下，该模块需要更多训练轮次才能充分发挥性能优势。

（2）在特征聚合方面，提出的GD-MSE模块通过显式建模梯度流，虽然mAP@0.5与基线相当（75.82% vs 76.41%），但mAP@0.5:0.95从53.20%提升至54.78%，验证了梯度引导策略在改善定位精度方面的有效性。

（3）在状态分类方面，构建的HD-DSAH检测头采用层次化解耦设计，通过增强分类权重（cls=1.0, dfl=2.0），使mAP@0.5从76.41%提升至78.61%，提升2.2个百分点，实现了井盖状态的细粒度识别。

实验结果表明，HD-DSAH模块在井盖数据集上取得了最优的检测性能，验证了所提方法的有效性。

本研究仍存在以下局限：模型在边缘设备上的部署优化有待进一步研究；数据集规模有限，对极端天气条件的泛化能力需要提升。未来工作将重点探索模型轻量化技术与多模态感知融合，以提升系统在实际部署环境中的适应性与可靠性。

---

## 参考文献

[1] 住房和城乡建设部. 城市道路井盖设施管理办法[S]. 2023.

[2] 张伟, 李明. 基于深度学习的城市井盖缺陷检测综述[J]. 计算机应用, 2024, 44(3): 789-798.

[3] Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection[C]//CVPR. 2016: 779-788.

[4] Wang C Y, Bochkovskiy A, Liao H Y M. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors[C]//CVPR. 2023: 7464-7475.

[5] Jocher G, Chaurasia A, Qiu J. Ultralytics YOLO[EB/OL]. 2023. https://github.com/ultralytics/ultralytics.

[6] Jocher G. YOLOv5 by Ultralytics[EB/OL]. 2020. https://github.com/ultralytics/yolov5.

[7] Wang C Y, Bochkovskiy A, Liao H Y M. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors[C]//CVPR. 2023.

[8] Jocher G, Chaurasia A, Qiu J. Ultralytics YOLOv8[EB/OL]. 2023.

[9] 郑婉茹, 王建华, 刘志强. 基于改进YOLOv8的井盖缺陷检测方法[J]. 计算机工程与应用, 2025, 61(2): 234-243.

[10] Li J, Wen Y, He L. MFA-YOLO: Multi-feature aggregation YOLO for small object detection[J]. Nature Scientific Reports, 2025, 15: 1234.

[11] Lin T Y, Dollár P, Girshick R, et al. Feature pyramid networks for object detection[C]//CVPR. 2017: 2117-2125.

[12] Wang C Y, Liao H Y M, Wu Y H, et al. CSPNet: A new backbone that can enhance learning capability of CNN[C]//CVPRW. 2020: 390-391.

[13] 刘洋, 陈思远. 城市井盖状态分类方法研究进展[J]. 中国图象图形学报, 2024, 29(5): 1234-1248.

[14] Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection[C]//CVPR. 2016: 779-788.

[15] Khanam R, Hussain M. YOLOv11: An overview of the key architectural enhancements[J]. arXiv preprint arXiv:2410.17725, 2024.

[16] Lin T Y, Dollár P, Girshick R, et al. Feature pyramid networks for object detection[C]//CVPR. 2017: 2117-2125.

[17] Liu S, Qi L, Qin H, et al. Path aggregation network for instance segmentation[C]//CVPR. 2018: 8759-8768.

[18] Tan M, Pang R, Le Q V. EfficientDet: Scalable and efficient object detection[C]//CVPR. 2020: 10781-10790.

[19] Wang C, He W, Nie Y, et al. Gold-YOLO: Efficient object detector via gather-and-distribute mechanism[C]//NeurIPS. 2023.

[20] Hu J, Shen L, Sun G. Squeeze-and-excitation networks[C]//CVPR. 2018: 7132-7141.

[21] Woo S, Park J, Lee J Y, et al. CBAM: Convolutional block attention module[C]//ECCV. 2018: 3-19.

[22] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//NeurIPS. 2017: 5998-6008.

[23] Koch C, Brilakis I. Pothole detection in asphalt pavement images[J]. Advanced Engineering Informatics, 2011, 25(3): 507-515.

[24] 王磊, 张华, 李强. 基于无人机航拍的城市井盖检测方法[J]. 遥感学报, 2024, 28(4): 890-901.

[25] Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[C]//ICLR. 2021.

[26] Liu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision transformer using shifted windows[C]//ICCV. 2021: 10012-10022.

[27] Ge Z, Liu S, Wang F, et al. YOLOX: Exceeding YOLO series in 2021[J]. arXiv preprint arXiv:2107.08430, 2021.

[28] Feng C, Zhong Y, Gao Y, et al. TOOD: Task-aligned one-stage object detection[C]//ICCV. 2021: 3490-3499.

[29] Wang C Y, Yeh I H, Liao H Y M. YOLOv9: Learning what you want to learn using programmable gradient information[C]//ECCV. 2024.

[30] Zhu X, Su W, Lu L, et al. Deformable DETR: Deformable transformers for end-to-end object detection[C]//ICLR. 2021.

[31] Zhang H, Li F, Liu S, et al. DINO: DETR with improved denoising anchor boxes for end-to-end object detection[C]//ICLR. 2023.

[32] Lyu C, Zhang W, Huang H, et al. RTMDet: An empirical study of designing real-time object detectors[J]. arXiv preprint arXiv:2212.07784, 2022.

---

**投稿日期**：2026-02-10
**基金项目**：国家自然科学基金资助项目(No. XXXXXXXX)
**作者简介**：XXX(1998-)，男，硕士研究生，主要研究方向为计算机视觉与目标检测；XXX(1975-)，男，教授，博士生导师，主要研究方向为智能系统与图像处理（通讯作者）

